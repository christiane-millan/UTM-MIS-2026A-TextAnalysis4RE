# Tokenizaci贸n

Los tokens son componentes textuales independientes y m铆nimos que tienen alguna sintaxis o sem谩ntica definida.

La tokenizaci贸n puede ser definida como el proceso de separa datos textuales en peque帽os componentes con significado llamados tokens.

## Tokenizaci贸n de palabras

La **tokenizaci贸n de palabras** es el proceso de separar o sementar oraciones en las palabras que la conforman. Una oraci贸n es una colecci贸n de palabras, y con la tokenizaci贸n se obtiene una lista de palabras a partir de la oraci贸n que pueden ser utilizadas para reconstruir la oraci贸n.

```python
from nltk.tokenize import word_tokenize
word_tokenize("Hi there!")
```

```
['Hi', 'there', '!']
```

La tokenizaci贸n de palabras es importante en muchos procesos, especialmente en la limpieza y normalizaci贸n de los textos donde operaciones como stemming o lematizaci贸n se aplica sobre cada palabra basado en sus respectivos stems (ra铆ces) o lemas. 

[Ejemplo 2. Tokenizaci贸n de palabras](./code/tokenization_words_nltk.ipynb)

## Otros tokenizadores en nltk

### Tokenizaci贸n de oraciones

La **tokenizaci贸n de oraciones** es el proceso de separar un corpus de texto en sentencias que act煤an como el primer nivel de tokens que componen el corpus.

NLTK (kit de herramientas de lenguaje natural, o m谩s com煤nmente NLTK) es un conjunto de bibliotecas y programas para el procesamiento del lenguaje natural (PLN) simb贸lico y estad铆sticos para el lenguaje de programaci贸n Python.

```python
from nltk.tokenize import sent_tokenize

texto = "La inteligencia artificial avanza r谩pidamente. Muchos investigadores trabajan en nuevos modelos."

oraciones = sent_tokenize(texto)
print(oraciones)
# Salida: ['La inteligencia artificial avanza r谩pidamente.', 
#          'Muchos investigadores trabajan en nuevos modelos.']
```

Existen varias formas de realizar la tokenizaci贸n de sentencias. Las t茅cnicas b谩sicas incluyen buscar delimitares espec铆ficos entre sentencias, como el punto y aparte `.` o un car谩cter de nueva l铆nea `\n`, y algunos otras veces punto y coma `;`.

[Ejemplo 1. Tokenizaci贸n de oraciones](./code/tokenization_sentences_nltk.ipynb)

### Tokenizaci贸n mediante expresiones regulares

La funci贸n `regexp_tokenize` permite dividir un texto en tokens (palabras, n煤meros o s铆mbolos) usando un patr贸n definido con expresiones regulares.

Esto le da flexibilidad al analista, pues puede decidir qu茅 extraer del texto seg煤n la necesidad.

```python
from nltk.tokenize import regexp_tokenize

texto = "La IA en 2025 es incre铆ble, 驴cierto?"

# Extraer solo palabras
tokens_palabras = regexp_tokenize(texto, pattern=r'\w+')
print(tokens_palabras)
# Salida: ['La', 'IA', 'en', '2025', 'es', 'incre铆ble', 'cierto']

# Extraer solo n煤meros
tokens_numeros = regexp_tokenize(texto, pattern=r'\d+')
print(tokens_numeros)
# Salida: ['2025']
```

### TweetTokenizer

El TweetTokenizer est谩 dise帽ado para trabajar con textos de Twitter.
Reconoce elementos que no suelen aparecer en otros tipos de documentos:

- Hashtags (#tema).
- Menciones (@usuario).
- Emojis y emoticonos.
- Contracciones y puntuaci贸n informal.

```python
from nltk.tokenize import TweetTokenizer

tweet = "Hola @Juan! Hoy lanzamos el nuevo modelo #IA2025 "

tokenizer = TweetTokenizer()
tokens = tokenizer.tokenize(tweet)
print(tokens)
# Salida: ['Hola', '@Juan', '!', 'Hoy', 'lanzamos', 'el', 'nuevo', 'modelo', '#IA2025', '', '']
```

### TreebankWordTokenizer

```python
from nltk.tokenize import TreebankWordTokenizer
tokenizer=TreebankWordTokenizer()
tokenizer.tokenize(corpus)
```