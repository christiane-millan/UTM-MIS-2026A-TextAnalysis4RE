{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización de palabras\n",
    "\n",
    "De forma similar a la tokenización de oraciones, nltk implementa distintas interfaces de tokenización de palabras, algunas de las interfaces principales son:\n",
    "\n",
    "* word_tokenize\n",
    "* TreebankWordTokenizer\n",
    "* RegexTokenizer\n",
    "* Tokenizadores inherentes de RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización mendiate `word_tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización con Spacy\n",
    "\n",
    "1. Instalar `spacy` v3.0\n",
    "2. Instalar el modelo de lenguaje `en_core_web_sm`: \n",
    "\n",
    "    ```` \n",
    "    python -m spacy download en_core_web_sm\n",
    "    ````\n",
    "\n",
    "3. En la versión 3.0 se nece\n",
    "\n",
    "    ````\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    ````\n",
    "\n",
    "4. Si en <v3.0 \n",
    "\n",
    "    ````\n",
    "    python -m spacy link en_core_web_sm en\n",
    "    ````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text_spacy = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The brown fox wasn't that quick and he couldn't win the race\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj.text for obj in text_spacy.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "print([obj.text for obj in text_spacy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización mediante `TreebankWordTokenizer` esta basada en Penn Treebank y utiliza varias expresiones regulares para tokenizar el texto. Es necesario considerar que ya se realizó previmante una tokenización por oraciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización mediante la clase `RegexpTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# pattern to identify tokens themselves\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pattern to identify gaps in tokens\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# get start and end indices of each token and then print them\n",
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print(word_indices)\n",
    "print([sentence[start:end] for start, end in word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen otras clases que realizan diferentes formas de tokenización: \n",
    "\n",
    "* La clase `WordPunktTokenizer` utiliza el patrón `r'\\w+|[^\\w\\s]+` para tokenizar sentencias en tokens independientes tanto alfabéticos como no alfabéticos.\n",
    "* La clase `WhitespaceTokenizer` tokeniza las sentencia en palabras basado en los espacios en blanco como tabulaciones, nuevas lineas y espacios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Tokenización\n",
    "\n",
    "Elige una canción en inglés y realiza primer la tokenización por oraciones. En seguida, al resultado aplica la tokenización por palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lyric = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Realizar la tokenización por oraciones, imprimir el número de oraciones y las primeras 5 oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_sentences ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Realizar la tokenización por palabras, imprimir el número de palabras de la primera oración y las palabras de dicha oración. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_words ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt4re2024A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
